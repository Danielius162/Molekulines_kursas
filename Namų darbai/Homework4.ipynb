{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Python for Data Analysis, GMC, Vilnius University, 2025\n",
        "\n",
        "# HW4: Training a succesfull machine learning (ML) model\n",
        "\n",
        "- **Tasks in this homework are built around a single data file** which should be downloaded in the Notebook as asked in the cell after the imports.\n",
        "- Packages allowed to be imported (but not necessarily needed): `scikit-learn`, `numpy`, `pandas`, `matplotlib`, `seaborn`, `tqdm`, `itertools`, `math`, `string`. Do not import any other packages.\n",
        "- **You will need to upload your solutions into your Github repository** dedicated for the Python for Data Analysis course. Use the same repository used for Homework 3.\n",
        "- Same requirements as for Homework 3:\n",
        "   - Do not write docstrings (function description comments).\n",
        "   - Keep prints informative.\n",
        "   - Do not create classes.\n",
        "   - Do not change assert statements.\n",
        "\n",
        "There are 5 tasks in this Notebook. They have slightly different numbers of points between them, with subpoints shown for each subtask e.g. (0.2p). You need to collect 8 points in total to get the maximum grade.\n",
        "\n",
        "As previously, each task consists of a text cell with task description, a code cell to solve the task, and a code cell with `assert` statements to check your code for *some* possible errors.\n",
        "\n",
        "Don't hesitate to contact me or Martynas if you are stuck."
      ],
      "metadata": {
        "id": "HCbuDJ9YfLr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your imports"
      ],
      "metadata": {
        "id": "kbIq-GLMmFHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the file using the link provided, by any method you like/understand the most,\n",
        "# but the downloading process should happen inside the Notebook.\n",
        "file_url = \"https://github.com/Tallivm/vu-python/blob/main/hw4_2025.csv\""
      ],
      "metadata": {
        "id": "rMmLobkRc7Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to store the name of the column to predict.\n",
        "# Use this variable when needed.\n",
        "TO_PREDICT = 'snail_genus'"
      ],
      "metadata": {
        "id": "fb2_t1bcmGLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‚ Task 1 (1 point): The new challenge"
      ],
      "metadata": {
        "id": "Bf-MilLshM9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some researchers who collected data for SNAILAB complained that measuring a single sample takes a lot of time and is quite difficult. Many snails are actually found on fallen leaves, not on whole plants, making several measurements impossible to make. Moreover, if a snail hides inside its shell, for certain measurements the researcher is forced to wait until the snail fully appears again.\n",
        "\n",
        "An intern from SNAILAB suggested that they could use AI to solve this issue. After several discussions, they decided to train a model which would **predict the genus of a snail** from only easy-to-measure features. The intern prepared some useful data but then got sick. So, SNAILAB asks for your help once more.\n",
        "\n",
        "You will need to train several simple models to predict snail genus for a provided data set, and select the best model.\n",
        "\n",
        "But first - the provided **data should be explored**!\n",
        "\n",
        "1. (0.2p) Load the data as a DataFrame, name it `raw`. Make sure the \"Unnamed: 0\" column is not formed by providing correct parameters into the `read_csv()` function. Print out a short report (in any format you like, make it a function) using f-strings and containing this information:\n",
        "   - Number of NaN values in total, and if there are NaNs, then in which columns and how many;\n",
        "   - Min, mean, and max values of each numeric column;\n",
        "   - Unique values and their counts of each categorical column.\n",
        "\n",
        "2. (0.4p) According to the report, make certain changes to the data and name the result `clean`:\n",
        "   - If there are NaNs, remove full rows with them;\n",
        "   - Remove full rows containing seemingly incorrect measurement values (e.g. negative values for length measurements).\n",
        "   - Even if these steps were not required for this data, do it nevertheless, in a way that could be applicable to any dataset with such requirements (but maybe different columns and values).\n",
        "\n",
        "3. (0.1p) Print out the report again using the previously written function.\n",
        "\n",
        "4. (0.3p) Obtain and visualize a Spearman correlation matrix (as a heatmap) for all numeric columns. Make sure colormap is used correctly (divergent, zero in the middle), and the plot contains column names."
      ],
      "metadata": {
        "id": "iebEKf1zgG86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "RhMB6yWmjsYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mamd7x8Be47r"
      },
      "outputs": [],
      "source": [
        "assert isinstance(raw, pd.DataFrame)\n",
        "assert isinstance(clean, pd.DataFrame)\n",
        "assert clean.isna().sum().sum() == 0\n",
        "assert len(raw.columns) == len(clean.columns)\n",
        "assert TO_PREDICT in raw.columns\n",
        "assert TO_PREDICT in clean.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¨ Task 2 (1 point): Data transformation and preparation for training"
      ],
      "metadata": {
        "id": "eFK3aoGrhOTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to prepare data for the model training. The DataFrame created during this task should be called `transformed`.\n",
        "\n",
        "1. (0.4p) Standardize numeric columns:\n",
        "   - using the `scikit-learn` package;\n",
        "   - using just `numpy`;\n",
        "   - Compare results and show that they are the same or similar enough. If there is any difference, notice how big it is.\n",
        "2. (0.3p) Encode all categorical columns except snail genus using one-hot encoding from `pandas` or `scikit-learn`. Make sure that in the end, there are N-1 columns for a categorical column with N unique values. Make sure that old categorical columns are not left in the data.\n",
        "3. (0.2p) Encode the snail genus as integer column using `pandas` or `numpy`. Make sure to create a dictionary `snail_classes` mapping snail genera and integers.\n",
        "4. (0.1p) Create `X` and `y` from the whole data. The `X` should contain all columns except the snail genus column, and the `y` should contain only the snail genus column and be a `Series` object."
      ],
      "metadata": {
        "id": "dyc7OWgFhW6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "lrNa5BIHj4TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(transformed, pd.DataFrame)\n",
        "assert transformed.isna().sum().sum() == 0\n",
        "assert len(transformed.columns) > len(clean.columns)\n",
        "assert TO_PREDICT in transformed.columns\n",
        "assert transformed[TO_PREDICT].dtype == int\n",
        "assert str not in transformed.dtypes  # CHECK IF WORKS\n",
        "assert 2 in transformed[TO_PREDICT]\n",
        "assert isinstance(X, pd.DataFrame)\n",
        "assert len(X.columns) == len(transformed.columns) - 1\n",
        "assert isinstance(y, pd.Series)\n",
        "assert isinstance(snail_classes, dict)"
      ],
      "metadata": {
        "id": "e-yDuzSij4L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ“ Task 3 (1 point): Model training and evaluation"
      ],
      "metadata": {
        "id": "zy4IVj2yhXai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function named `split_and_train_model` which uses the standard model training pipeline:\n",
        "- It should take `X`, `y`, `random_seed`, and a function (type `Callable`) to create the model. It should also take an optional `max_iter` parameter with default value of 300.\n",
        "- Inside, it should:\n",
        "   - Correctly split `X` and `y` into `X_train`, `y_train`, `X_test`, `y_test`. You can use different names but the structure should remain the same. Use test size of 20%. Use `random_seed` to fix the random state of data splitting.\n",
        "   - Create an instance of the chosen model (by calling the provided function), with its random seed fixed to `random_seed` parameter.\n",
        "   - Use the model to fit `X_train` and `y_train`.\n",
        "   - Use the fitted model to generate predictions from `X_test`.\n",
        "   - Calculate the accuracy score by comparing `y_test` and obtained predictions. Print out the score (formatted using f-string).\n",
        "   - Return the trained model.\n",
        "   - In case of **any** exception, do not raise it, but print out the error message and return `None` instead.\n",
        "\n",
        "As an usage example, use this function with the data prepared in Task 2 and `LogisticRegression` from `scikit-learn`."
      ],
      "metadata": {
        "id": "RWgPK0cxhh68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "N1Kabl8WhUt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no asserts there!"
      ],
      "metadata": {
        "id": "kg7sLMD8o5wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§© Task 4 (2 points): Feature extraction - clustering"
      ],
      "metadata": {
        "id": "jslHSd-Mhnk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Someone from SNAILAB theorized that knowing the genus of the plant which the leaf belongs to should help predict the genus of the snail, as certain snails are attracted to certain plants. However, the dataset does not contain plant names, and either way, plant identification requires additional time and skill.\n",
        "\n",
        "Instead, you can use unsupervised learning to cluster plant features and use this information as a new feature.\n",
        "\n",
        "1. (0.1p) Create a new DataFrame `plants` containing only plant features from `transformed`. Here, you are allowed to write column names manually.\n",
        "2. (0.9p) You will use several clustering methods: [Affinity Propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html), [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html), and [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). They allow to provide different clustering parameters. Create a list `cluster_setups` containing `tuple[str, Callable, dict]` items (setups), where `str` is a short model name just for printing purposes, `Callable` is the clustering function (which you will call), and `dict` is a dictionary of 1-2 parameters to provide to that function.\n",
        "   - There should be several setups for each clustering method. Use `for` loops to create those setups automatically by going through lists of possible parameters. You are allowed to write the lists manually or use `np.linspace` and similar functions:\n",
        "      - For Affinity Propagation, use 2-3 different `damping` values between 0.6 and 0.9;\n",
        "      - For Spectral Clustering and KMeans, use 4-5 `n_clusters` values between 3 and 20.\n",
        "      - For DBSCAN, use 4-5 `eps` values between 0.1 and 0.5, and 4-5 `min_samples` values between 5 and 40 (so each DBSCAN setup had two provided parameters instead of one).\n",
        "3. (1.0p) For each setup in `cluster_setups`, fit a clustering model on `plants` data and get the preidcted labels for all plants. Save the labels into a dictionary `obtained_clusters` which should be of type `dict[str, list]`, The `str` keys should be some kind of automatically generated short model descriptions (e.g. use f-string and include used parameter values in it). The `list` values should be lists of predicted cluster labels.\n",
        "   - You may want to use `tqdm` at this point, as some clustering methods are slower."
      ],
      "metadata": {
        "id": "I42INxTThxf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "B7fvuZXGhy1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(plants, pd.DataFrame)\n",
        "assert isinstance(cluster_setups, list)\n",
        "assert isinstance(obtained_clusters, dict)\n",
        "assert len(cluster_setups) == len(obtained_clusters)\n",
        "assert len(cluster_setups) >= 26"
      ],
      "metadata": {
        "id": "pTzjEaMoqz3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‘‘ Task 5 (2 points): Using extracted features to improve the result"
      ],
      "metadata": {
        "id": "5uKxDGaZxb0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only thing left now is to check which clustering setup produced a new feature (predicted plant clusters) which improves the Logistic Regression model trained in Task 3.\n",
        "\n",
        "1. (1.0p) For each plant clustering result from Task 4, check if it improves the accuracy of logistic regression:\n",
        "   - Create a new variable `XX` containing the `X` from Task 2 but joined with the new feature. If the shape of `X` was (M, N), then the shape of `XX` should be (M, N+1).\n",
        "   - Using already written `split_and_train_model` function, create and fit a new logistic regression model on `XX` (`y` remains unchanged from Task 2). You may need to increase `max_iter` here. Don't forget to use the same random seed for all models.\n",
        "2. (1.0p) Automatically find the feature which produced the best result from all trained Logistic Regression models. Print out its name and received accuracy score.\n"
      ],
      "metadata": {
        "id": "xuAg6E1bqzuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "EhJD1ZihyCob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no asserts again!"
      ],
      "metadata": {
        "id": "JTPjKei9yD3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}